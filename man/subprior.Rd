% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Subprior.R
\name{subprior}
\alias{subprior}
\title{Consensus Algorithm for Parameter Estimation}
\usage{
subprior(
  chains,
  para,
  startval,
  niter,
  X,
  prior,
  likelihood,
  propvar = NULL,
  burn.rate = 0.1,
  random = T
)
}
\arguments{
\item{chains}{Number of subsets in the simulation. Used when a divide-and-conquer algorithm is employed.}

\item{para}{Parameters to be estimated.}

\item{startval}{Initial value of the chain.}

\item{niter}{Number of iterations (including burned iterations).}

\item{X}{Matrix of observations from the underlying data set.}

\item{prior}{Prior function for the parameters.}

\item{likelihood}{Likelihood function.}

\item{propvar}{The diagonal of the variance matrix for the proposal distribution. If no value is specified, the identify matrix is used.}

\item{burn.rate}{The percentage of iterations to be burned.}

\item{random}{If true, the rows of X are shuffled prior to the split.}
}
\value{
A list with the following items:
\describe{
\item{\code{Chains}}{A list of dataframes, one for each subset, containing the generated Markov chains.}
\item{\code{Estimate}}{The paraeter estimate obtained using the consensus algorithm}
}
}
\description{
This function estimates the paraeters of a model using the consensus algorithm method developped by Scott et al. (2016). The consensus algorithm models the subposterior of subset \eqn{X_k} as
\deqn{\pi_k(\theta | X_k) ∝ \pi(\theta)^(1/K) \prod p(x|\theta)}
Parameter estimates are subsequently weighted according to the variance of their Markov chains.
}
\examples{
#Parameter estimation for data from a normal distribution

#Prior
prior<-function(param){
  ifelse(param[2]>0, 1, 0)
}

#Likelihood
normal.likelihood<-function(X, param){
  mu=param[1]
  sigma=param[2]
  sum(dnorm(x=X, mean=mu, sd=sigma, log=T))
}

#Simulate data
X<-rnorm(100, -0.5, 0.5)

#Parameters
param<-c("mu", "sigma")
niter<-2500
startval<-c(0, 5)

df<-subprior(chains=4,para=param, startval=startval, niter=niter, X=X,
             prior=prior, likelihood=normal.likelihood, propvar=0.25, random=TRUE)

}
\references{
Steven L. Scott, Alexander W. Blocker, Fernando V. Bonassi, Hugh A. Chipman, Edward I. George, and Robert McCulloch. Bayes and big data: The consensus monte carlo algorithm. International Journal of Management Science and Engineering Management, 11(2):78–88, 2016.
}
